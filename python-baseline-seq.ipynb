{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34f13e5d-1b16-4d95-a285-12b2ee31a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26d5b5c1-c90e-417e-a6a0-3f4e1f4e242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of sentences as our corpus\n",
    "corpus = ['this is a a sample', 'this is another another example example example']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aabb706a-3a02-4ead-9775-78905635a696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample', 'is', 'a', 'another', 'this', 'example']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a set of unique words from the corpus by iterating over each sentence and each word in the sentence\n",
    "vocabs = list(set([word for doc in corpus for word in doc.split()]))\n",
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0403433-c113-4fd2-b0b4-7793a2658b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping each unique word to an integer index\n",
    "word2id = {w:i for i, w in enumerate(vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23c4c47a-1e91-4efd-a0e9-00395505e7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences in the corpus by replacing each word in the sentence with its corresponding integer index\n",
    "corpus = [[word2id[word] for word in doc.split()] for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efa65bd3-32fe-4056-855a-ddf11ad80041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 1, 2, 2, 0], [4, 1, 3, 3, 5, 5, 5]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7a235af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('untitled.txt') as fp:\n",
    "#     corpus = []\n",
    "#     for line in fp:\n",
    "#         corpus.append([int(i) for i in line.split()])\n",
    "#     vocab_size = max([max(i) for i in corpus])+1\n",
    "#     vocabs = [1 for i in range(vocab_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9094f6ba-eb5f-46b2-86ec-34da69518e00",
   "metadata": {},
   "source": [
    "# Definition TF-IDF\n",
    "1. The tfâ€“idf is the product of two statistics, term frequency and inverse document frequency. There are various ways for determining the exact values of both statistics.\n",
    "2. A formula that aims to define the importance of a keyword or phrase within a document or a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0d5acc1-56aa-4004-8c40-c3c289552408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_size = len(corpus)\n",
    "vocab_size = len(vocabs)\n",
    "counts = np.zeros((doc_size, vocab_size)) # when using ndarray it return the same address and the previous values\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e945fe2-aade-4907-8e21-7982d56f69e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(doc_size):\n",
    "    for word in corpus[i]:\n",
    "        counts[i][word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b37dcee6-1321-43d5-a7ae-84bfed39e595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 2., 0., 1., 0.],\n",
       "       [0., 1., 0., 2., 1., 3.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5dfdf01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 7.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64621e48-636e-4d73-9d86-73cb769167ae",
   "metadata": {},
   "source": [
    "## Term frequency\n",
    "Term frequency, tf(t,d), is the relative frequency of term t within document d,\n",
    "$$\n",
    "{\\displaystyle \\mathrm {tf} (t,d)={\\frac {f_{t,d}}{\\sum _{t'\\in d}{f_{t',d}}}}},\n",
    "$$\n",
    "where $f_{t,d}$ is the raw count of a term in a document, i.e., the number of times that term $t$ occurs in document $d$. Note the denominator is simply the total number of terms in document $d$ (counting each occurrence of the same term separately). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68469560-d94a-4c29-996f-258e9d778df5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2        0.2        0.4        0.         0.2        0.        ]\n",
      " [0.         0.14285714 0.         0.28571429 0.14285714 0.42857143]]\n"
     ]
    }
   ],
   "source": [
    "tf = np.zeros_like(counts)\n",
    "for i in range(doc_size): # O(nxm)\n",
    "    wc_sum = 0\n",
    "    for wc in counts[i]: wc_sum += wc # O(m)\n",
    "    for j in range(vocab_size): # word count in doc O(m)\n",
    "        tf[i][j] = counts[i][j]/wc_sum\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa01b2db-f9f8-43ff-8080-f8012a842bb4",
   "metadata": {},
   "source": [
    "## The inverse document frequency\n",
    "The inverse document frequency is a measure of how much information the word provides, i.e., if it is common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient):\n",
    "\n",
    "$$\n",
    "\\mathrm{idf}(t, D) =  \\log \\frac{N}{|\\{d \\in D: t \\in d\\}|}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "- N: total number of documents in the corpus $N = {|D|}$\n",
    "\n",
    "- $|\\{d \\in D: t \\in d\\}|$  : number of documents where the term $mathrm{tf}(t,d) \\neq 0)$. If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to $1 + |\\{d \\in D: t \\in d\\}|.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4e7a449-99e4-4ed9-af21-caefe189f92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30103, 0.     , 0.30103, 0.30103, 0.     , 0.30103])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = np.zeros(vocab_size)\n",
    "for i in range(vocab_size):\n",
    "    w_dc = 0 # word found in `w_dc` docs, start with 1 to prevent division-by-zero\n",
    "    for j in range(doc_size):\n",
    "        if counts[j][i] != 0: w_dc += 1\n",
    "    idf[i] = np.log10(doc_size/w_dc)\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35e3024d-f0b7-47ab-a67a-cb59e30943d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.060206  , 0.        , 0.120412  , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.08600857, 0.        ,\n",
       "        0.12901286]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37863896-8d7f-414e-af39-f0ebaaa15717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce0daf84-54e9-45bb-97b6-32b477591e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "636d2437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** 17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
